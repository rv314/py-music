{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generation using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a RNN for music generation using PyTorch. ABC notations are being used for this training, since they are in text format, they are suitable to use to train a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMET ML to track model experiments\n",
    "import comet_ml\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Remaining imports\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import functools\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 COMET API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "comet_api_key = os.getenv('COMET_API_KEY')\n",
    "\n",
    "# Validation\n",
    "assert comet_api_key is not None, \"Please set COMET_API_KEY in .env file\"\n",
    "# TODO: Need to setup CUDA\n",
    "assert torch.cuda.is_available(), \"CUDA not available\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "s = load_data()\n",
    "\n",
    "# Print one of the samples\n",
    "example = s[0]\n",
    "print(\"Example of a sample:\")\n",
    "print(example)\n",
    "\n",
    "play_song(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all the lyrics, sort and de-duplicate\n",
    "all_lyrics = \"\\n\\n\".join(s)\n",
    "\n",
    "# Unique characters\n",
    "vocab = sorted(set(all_lyrics))\n",
    "\n",
    "print(f\"Unique characters in the dataset: {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to convert characters to indices\n",
    "char_to_index = {c: i for i, c in enumerate(vocab)}\n",
    "\n",
    "# Simple function to convert indices to characters\n",
    "index_to_char = {i: c for i, c in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "#print(f\"index_to_char: {len(index_to_char)}\")\n",
    "#print(f\"idx2char: {len(idx2char)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{')\n",
    "for char, _ in zip(char_to_index, range(50)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char_to_index[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vectorize the songs string ###\n",
    "\n",
    "'''TODO: Write a function to convert the all songs string to a vectorized\n",
    "    (i.e., numeric) representation. Use the appropriate mapping\n",
    "    above to convert from vocab characters to the corresponding indices.\n",
    "\n",
    "  NOTE: the output of the `vectorize_string` function\n",
    "  should be a np.array with `N` elements, where `N` is\n",
    "  the number of characters in the input string\n",
    "'''\n",
    "def vectorize_string(string):\n",
    "    return np.array([char_to_index[c] for c in string])\n",
    "  \n",
    "\n",
    "vectorized_songs = vectorize_string(all_lyrics)\n",
    "# print(vectorized_songs[:100])\n",
    "assert isinstance(vectorized_songs, np.ndarray), \"returned result should be a np array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('{} ---- characters mapped to int ----> {}'.format(repr(all_lyrics[20:50]), vectorized_songs[20:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_songs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Function to create batches of data based on sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Batch definition to create training examples ###\n",
    "\n",
    "def get_batch(vectorized_songs, seq_length, batch_size):\n",
    "  # the length of the vectorized songs string\n",
    "  n = vectorized_songs.shape[0] - 1\n",
    "  # randomly choose the starting indices for the examples in the training batch\n",
    "  idx = np.random.choice(n-seq_length, batch_size)\n",
    "\n",
    "  # TODO: construct a list of input sequences for the training batch\n",
    "  input_batch = [vectorized_songs[i : i+seq_length] for i in idx]\n",
    "\n",
    "  # TODO: construct a list of output sequences for the training batch\n",
    "  output_batch = [vectorized_songs[i+1 : i+seq_length+1] for i in idx]\n",
    "\n",
    "  # x_batch, y_batch provide the true inputs and targets for network training\n",
    "  # Creating a tensor from a list of numpy.ndarrays is extremely slow. \n",
    "  # Converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor.\n",
    "  #x_batch = np.array(input_batch)\n",
    "  #y_batch = np.array(output_batch)\n",
    "  x_batch = torch.tensor(np.array(input_batch), dtype=torch.long)\n",
    "  y_batch = torch.tensor(np.array(output_batch), dtype=torch.long)\n",
    "\n",
    "  return x_batch, y_batch\n",
    "\n",
    "# Simple tests\n",
    "args = (vectorized_songs, 10, 2)\n",
    "x_batch, y_batch = get_batch(*args)\n",
    "print(\"Input: \", x_batch)\n",
    "print(\"Target: \", y_batch)\n",
    "\n",
    "assert x_batch.shape == (2, 10), \"Incorrect batch shape\"\n",
    "assert y_batch.shape == (2, 10), \"Incorrect batch shape\"\n",
    "print(\"Passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "   input: 14 ('2')\n",
      "   expected output: 26 ('A')\n",
      "Step 1\n",
      "   input: 26 ('A')\n",
      "   expected output: 14 ('2')\n",
      "Step 2\n",
      "   input: 14 ('2')\n",
      "   expected output: 82 ('|')\n",
      "Step 3\n",
      "   input: 82 ('|')\n",
      "   expected output: 27 ('B')\n",
      "Step 4\n",
      "   input: 27 ('B')\n",
      "   expected output: 26 ('A')\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
    "\n",
    "for i, (input_idx, target_idx) in enumerate(zip(x_batch[0], y_batch[0])):\n",
    "    print(f\"Step {i}\")\n",
    "    print(f\"   input: {input_idx} ({repr(idx2char[input_idx.item()])})\")\n",
    "    print(f\"   expected output: {target_idx} ({repr(idx2char[target_idx.item()])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Defining the RNN ####\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "    super(LSTMModel, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    # embedding layer\n",
    "    # Layer 1: Embedding layer to transform indices into dense vectors\n",
    "    # of fixed size (embedding_dim)\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    # LSTM layer\n",
    "    # Layer 2: LSTM with `hidden_size` number of hidden units\n",
    "    self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "\n",
    "    # Layer 3: Linear layer (fully connected layer) that maps the LSTM layer's output\n",
    "    # to the number of characters we have in our vocabulary\n",
    "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "      # Initialize hidden and cell states\n",
    "      return (torch.zeros(1, batch_size, self.hidden_size).to(device),\n",
    "              torch.zeros(1, batch_size, self.hidden_size).to(device))\n",
    "    \n",
    "    def forward(self, x, state=None, return_state=False):\n",
    "      x = self.embedding(x)\n",
    "\n",
    "      if state is None:\n",
    "        state = self.init_hidden(x.size(0), x.device)\n",
    "      out, state = self.lstm(x, state)\n",
    "\n",
    "      out = self.linear(out)\n",
    "      return out if not return_state else (out, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
